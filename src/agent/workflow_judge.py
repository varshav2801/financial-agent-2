"""Workflow judge for auditing execution results against document sources"""
from typing import Optional, TYPE_CHECKING
from pydantic import BaseModel, Field
from src.models.workflow_schema import WorkflowPlan, StepCritique
from src.models.dataset import Document
from src.services.llm_client import get_llm_client
from src.logger import get_logger
from src.prompts.result_verifier import RESULT_VERIFIER_SYSTEM_PROMPT

if TYPE_CHECKING:
    from src.services.llm_client import LLMClient

logger = get_logger(__name__)


class StepAudit(BaseModel):
    """Audit result for a single execution step"""
    step_id: int = Field(description="Step ID being audited")
    is_grounded: bool = Field(
        description="False only if the value directly contradicts the document source"
    )
    critique: Optional[str] = Field(
        None, 
        description="Specific reason for the failure (required if is_grounded=False)"
    )


class JudgeAudit(BaseModel):
    """Complete audit result from the judge"""
    is_valid: bool = Field(
        description="Judge's assessment: False if grounding errors found, True otherwise (NOT a retry decision)"
    )
    confidence_score: int = Field(
        ge=0, 
        le=100,
        description="Confidence in the audit judgment (0-100)"
    )
    step_audits: list[StepAudit] = Field(
        default_factory=list,
        description="Per-step audit results"
    )
    overall_reasoning: str = Field(
        description="Brief explanation of why the answer is acceptable or not"
    )


class WorkflowJudge:
    """
    Post-execution auditor that verifies semantic grounding of extraction steps.
    
    Key Design:
    - Does NOT check arithmetic (executor is deterministic)
    - ONLY flags semantic mismatches (wrong year, wrong entity, wrong unit)
    - Returns StepCritique format compatible with planner refinement loop
    """
    
    def __init__(self, llm_client: Optional["LLMClient"] = None) -> None:
        self.llm_client = llm_client if llm_client is not None else get_llm_client()
    
    async def evaluate(
        self,
        question: str,
        plan: WorkflowPlan,
        execution_trace: dict[int, float],
        document: Document,
        previous_answers: dict[str, dict] | None = None
    ) -> JudgeAudit:
        """
        Audit execution results for semantic grounding errors.
        
        Args:
            question: Original user question
            plan: Generated workflow plan
            execution_trace: Execution memory (step_id -> result value)
            document: Source document with table and text
            previous_answers: Conversation history with metadata (optional)
            
        Returns:
            JudgeAudit with grounding validation results
        """
        # Format execution trace for readability
        trace_str = "\n".join([
            f"  Step {step_id}: {value}"
            for step_id, value in sorted(execution_trace.items()) if step_id > 0
        ])
        
        # Format table structure
        table_str = self._format_table_preview(document)
        
        # Format conversation history
        conversation_context = "No previous turns (first question in conversation)"
        if previous_answers:
            history_lines = []
            for key in sorted(previous_answers.keys(), key=lambda k: int(k.split('_')[1])):
                ans_data = previous_answers[key]
                if isinstance(ans_data, dict):
                    entity = ans_data.get('entity', 'unknown')
                    operation = ans_data.get('operation', 'unknown')
                    value = ans_data.get('value', 'N/A')
                    question_text = ans_data.get('question', 'N/A')
                    history_lines.append(
                        f"  {key}: {value} (entity: {entity}, operation: {operation})\n"
                        f"    Question was: {question_text}"
                    )
                else:
                    history_lines.append(f"  {key}: {ans_data}")
            conversation_context = "\n".join(history_lines)
        
        user_message = f"""Question: {question}

Planner's Reasoning (thought_process):
{plan.thought_process}

Final Answer: {execution_trace.get(max([k for k in execution_trace.keys() if k > 0], default=1))}

Execution Trace (Step ID → Result Value):
{trace_str}

Conversation History:
{conversation_context}

Workflow Plan (Generated by Planner):
{plan.model_dump_json(indent=2)}

Document Sources:
{table_str}

Pre-Text Context (first 800 chars):
{document.pre_text[:800]}

Post-Text Context (first 800 chars):
{document.post_text[:800]}

====================
AUDIT TASK
====================
Verify that each extraction step (tool="extract_value") pulled data from the CORRECT source.

**CRITICAL - Read the Planner's Reasoning First:**
The planner's thought_process explains its interpretation of the question. Use this to understand:
- What the planner thinks the question is asking
- Which entities/years it decided to extract
- Why it chose to use previous answers (step_ref: -1, -2) vs. new extractions

**Pronoun Resolution Guidelines:**
- "what is the difference?" after two extractions → VALID to subtract previous answers
- "what about [entity]?" → VALID to use conversation history entity
- "this value" / "that year" → Check conversation history to resolve reference
- If planner's thought_process explains the reference logically, ACCEPT IT

**Only Flag These Clear Errors:**
1. Wrong year: col_query="2022" when question explicitly says "2023"
2. Wrong entity: row_query="Net Income" when question explicitly says "Operating Income"
3. Wrong reference: Uses prev_0 when that entity doesn't match current question
4. Unit mismatch: Document uses billions but parameters say millions

**Do NOT Flag:**
- Using previous answers when question is contextual ("what is the difference?")
- Pronouns that logically reference conversation history
- Plans where thought_process provides valid reasoning for the approach

For each failed step, provide a SPECIFIC critique:
1. Exact parameter that's wrong (e.g., "col_query='2022'" when should be "col_query='2023'")
2. What the question explicitly asked for vs. what the plan extracted
3. The correct parameter value to use instead

Example good critique: "Step 2 extracted col_query='2022' but question asks 'what was the 2023 value?' - should use col_query='2023'"
Example bad critique: "Wrong year extracted"
"""

        logger.info(f"Auditing execution for question: {question[:50]}...")
        
        response = await self.llm_client.parse_completion(
            messages=[
                {"role": "system", "content": RESULT_VERIFIER_SYSTEM_PROMPT},
                {"role": "user", "content": user_message}
            ],
            response_format=JudgeAudit
        )
        
        audit_result = response.choices[0].message.parsed
        
        if audit_result.is_valid:
            logger.info(f"✓ Judge audit passed: {audit_result.overall_reasoning}")
        else:
            failed_steps = [sa.step_id for sa in audit_result.step_audits if not sa.is_grounded]
            logger.warning(
                f"✗ Judge audit failed: {len(failed_steps)} grounding errors "
                f"(confidence: {audit_result.confidence_score}%)"
            )
            for sa in audit_result.step_audits:
                if not sa.is_grounded:
                    logger.warning(f"  Step {sa.step_id}: {sa.critique}")
        
        return audit_result
    
    def should_retry(self, audit: JudgeAudit, confidence_threshold: int = 80) -> bool:
        """
        Determine if audit should trigger a retry based on confidence threshold.
        
        Args:
            audit: Judge audit result
            confidence_threshold: Minimum confidence to trigger retry (default: 80)
            
        Returns:
            True if audit failed AND confidence >= threshold
        """
        return not audit.is_valid and audit.confidence_score >= confidence_threshold
    
    def convert_to_critiques(self, audit: JudgeAudit) -> list[StepCritique]:
        """
        Convert JudgeAudit to StepCritique format for planner refinement.
        
        Args:
            audit: Judge audit result
            
        Returns:
            List of StepCritique objects for failed steps
        """
        critiques = []
        
        for step_audit in audit.step_audits:
            if not step_audit.is_grounded and step_audit.critique:
                # Extract the specific fix from the critique if possible
                # Judge should provide format like: "should use col_query='2023'"
                fix_suggestion = step_audit.critique
                
                # If critique doesn't already include "should" or "use", add actionable instruction
                if "should" not in step_audit.critique.lower() and "use" not in step_audit.critique.lower():
                    fix_suggestion = (
                        f"{step_audit.critique}. "
                        f"Re-check extraction parameters for Step {step_audit.step_id} and correct the mismatch."
                    )
                
                critiques.append(StepCritique(
                    step_id=step_audit.step_id,
                    issue_type="GroundingError",
                    reason=step_audit.critique,
                    fix_suggestion=fix_suggestion
                ))
        
        # Add overall context if available
        if not audit.is_valid and audit.overall_reasoning and not critiques:
            # Fallback if no specific step audits but overall failure
            critiques.append(StepCritique(
                step_id=None,
                issue_type="SemanticMismatch",
                reason=audit.overall_reasoning,
                fix_suggestion=(
                    "Review the entire plan to ensure extracted data semantically matches "
                    "the question's intent. Check for correct entities, years, and data sources."
                )
            ))
        
        return critiques
    
    def _format_table_preview(self, document: Document) -> str:
        """Format table structure for judge context"""
        if not document.table:
            return "No table available"
        
        # Get table structure
        lines = ["Table Structure:"]
        
        # Show available columns/years
        first_row = next(iter(document.table.values()), {})
        if first_row:
            years = list(first_row.keys())
            lines.append(f"  Available Years/Columns: {', '.join(years)}")
        
        # Show available rows (first 10)
        rows = list(document.table.keys())[:10]
        lines.append(f"  Available Rows (first 10): {', '.join(rows)}")
        
        # Show sample values from first 3 rows
        lines.append("\n  Sample Values:")
        for row in list(document.table.keys())[:3]:
            row_data = document.table[row]
            sample_vals = list(row_data.items())[:3]
            vals_str = ", ".join([f"{k}: {v}" for k, v in sample_vals])
            lines.append(f"    {row}: {vals_str}")
        
        return "\n".join(lines)
